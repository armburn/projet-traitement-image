{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klwf9CMMqK_X"
      },
      "source": [
        "# SR Example\n",
        "\n",
        "First be sure the notebook is running with GPU.\n",
        "\n",
        "Runtime -> Change Runtime Type -> GPU\n",
        "\n",
        "\n",
        "***Le code fourni au départ est celui pour faire la Super Résolution***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TeMgjXqvGxp"
      },
      "source": [
        "# Charger les bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDhltdotcvAG",
        "outputId": "85cdf803-1b6f-4e2e-da9e-013d7e898052"
      },
      "outputs": [],
      "source": [
        "# Colab only\n",
        "# download and install Pytorch\n",
        "\n",
        "# ferdi 1\n",
        "\n",
        "!pip install torch torchvision\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "print(f\"Python version: {platform.python_version()}\")\n",
        "\n",
        "import torch\n",
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQWC6Ej05QsL"
      },
      "outputs": [],
      "source": [
        "# import the packages\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import PyTorch (Deep Learning lib)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import math\n",
        "import inspect  # debugage\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ctDUoJs8qk"
      },
      "source": [
        "# La Base de donnée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-s4WyZ_tEQH"
      },
      "source": [
        "On a choisit de travailler avec CIFAR 10, des images RGB 32 par 32, avec 10 classes, 50000 données d'entrainement et 10000 données de test.\n",
        "\n",
        "\n",
        "On a voulu au départ travailler avec la base de donnée CELEBA, mais celle si étant trop lourde, avec plus de 2000 classes, on s'est retrouvé face aux problèmes suivants :\n",
        "- devoir charger les données prend beaucoup de temps (+ de 200000 images)\n",
        "- le téléchargement devait se faire sur un lien drive qui autorisait un download par 24h\n",
        "- google collab enregistre le code mais vide les fichiers entre chaque session, on a donc tout perdu après les efforts\n",
        "\n",
        "On a choisit une solutions sans drive, pytorch offre directement un accès à la base de donnée CIFAR10, qu'on peut donc download a chaque session sans difficulté et rapidement (car plus légère).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZHM_ZfY64Gz",
        "outputId": "d5765f95-5563-41ad-a526-67e7838cf7e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "print(trainset)\n",
        "\n",
        "print(testset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2C__SInyS8I"
      },
      "source": [
        "On utilise une transformation pour normaliser les images $I \\in [0, 1]^3$ de CIFAR-10 de façon à avoir $I_{norm} \\in [-1, 1]^3$ :\n",
        "\n",
        "$\n",
        "I_{norm} = \\frac{I - \\mu}{\\sigma}\n",
        "$\n",
        "\n",
        "d'ou $\\mu = 0.5$ et $\\sigma = 0.5$\n",
        "\n",
        "La normalisation vise à centrer les valeurs autour de zéro et à les mettre à l'échelle de manière à avoir une variance relativement uniforme. Cela peut aider à stabiliser et à accélérer l'entraînement du modèle, en particulier lorsque vous utilisez des méthodes d'optimisation sensibles à l'échelle des données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "Wz6Y30RXsuGE",
        "outputId": "68d0a201-8eb3-4bf6-e2cd-d26d14a3ca7c"
      },
      "outputs": [],
      "source": [
        "# Fonction pour afficher le nombre d'images par classe\n",
        "def show_class_distribution(dataset, set_name):\n",
        "    class_counts = [0] * len(classes)\n",
        "    for _, label in dataset:\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(classes, class_counts)\n",
        "    ax.set_xlabel('Classes')\n",
        "    ax.set_ylabel('Nombre d\\'images')\n",
        "    ax.set_title(f'Distribution des classes dans l\\'ensemble {set_name}')\n",
        "    plt.show()\n",
        "\n",
        "# Afficher la distribution des classes dans l'ensemble d'entraînement\n",
        "show_class_distribution(trainset, \"d'entraînement\")\n",
        "\n",
        "# Afficher la distribution des classes dans l'ensemble de test\n",
        "show_class_distribution(testset, \"de test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "2GWUrKvF6-fE",
        "outputId": "fab2fab0-d756-4939-dec5-4e96394d1dc5"
      },
      "outputs": [],
      "source": [
        "# afficher un batch : (cette case peut être run autant de fois qu'on veut)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print(f\"batch_size = {images.shape[0]} et dimension d'une image = {images.shape[1:]}\")\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoTkbwVqzax"
      },
      "source": [
        "# Le modèle simple\n",
        "\n",
        "Dans cette partie, on va suivre le tutoriel de classification de pytorch (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcOEoWQwuTwZ"
      },
      "source": [
        "## Création du Réseau :\n",
        "\n",
        "Dans le tuto, on utilise la structure suivante :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuQodHe6o2n6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class S_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## conv1 puis pool :\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        ## conv2 puis pool :\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        ## flatten :\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        ## fully connected 1 :\n",
        "        x = F.relu(self.fc1(x))\n",
        "        ## fully connected 2 :\n",
        "        x = F.relu(self.fc2(x))\n",
        "        ## fully connected 3 :\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def size_debug(self, x):\n",
        "        forward_source = inspect.getsource(self.forward)\n",
        "        forward_lines = forward_source.split('\\n')[1:-2]  # Skip def and return\n",
        "        local_vars = locals()\n",
        "        print(\"in :\")\n",
        "        print(x.shape)\n",
        "        for line in forward_lines:\n",
        "            if line.strip():\n",
        "                if line.strip()[0] != '#':\n",
        "                    exec(line.strip(), globals(), local_vars)\n",
        "                    x = local_vars['x']\n",
        "                    print(x.shape)\n",
        "                elif line.strip()[0:2] == '##':\n",
        "                    print(line.strip())\n",
        "        print(\"out.\")\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.datasets import fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "\n",
        "net = S_Net()\n",
        "net = net.cuda()\n",
        "\n",
        "net.size_debug(images.cuda())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyGUwsq1syZ7"
      },
      "source": [
        "## Optimization :\n",
        "\n",
        "Dans le tuto, on utilise comme fonction de permet la Cross Entropy, et comme algorithme d'optimisation, la descente de  gradient stochastique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7AjcUhqqm58"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFfaiUeZquj1",
        "outputId": "b2419161-ff74-4129-8d63-60ce27d71c8e"
      },
      "outputs": [],
      "source": [
        "n_epochs = 2\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ioiak4Q7q2M7"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "PATH = f\"./cifar_net_tuto_n_epochs={n_epochs}.pth\"\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIhYrnBuieu"
      },
      "source": [
        "## Evaluation du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Z4gy06r4rh-n",
        "outputId": "2b37d292-a6b2-4cff-d432-5ae2967bd5ef"
      },
      "outputs": [],
      "source": [
        "# tester le modele : (cette case peut être run autant de fois qu'on veut)\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "\n",
        "net = Net()\n",
        "net = net.cuda()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "outputs = net(images.cuda())\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcJ_ozVMr3y4",
        "outputId": "61213d6b-3dc0-4dd3-963a-6fcb888eaa4a"
      },
      "outputs": [],
      "source": [
        "# tester sur tout le test dataset :\n",
        "\n",
        "net = Net()\n",
        "net = net.cuda()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the {len(testset)} test images: {100 * correct // total} %')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "CThegUeisGMT",
        "outputId": "b8932fd3-6361-4d92-cb7e-3cd486504c76"
      },
      "outputs": [],
      "source": [
        "# précision en fonction des classes :\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "\n",
        "noms_classe = []\n",
        "precisions_classe = []\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "    noms_classe.append(classname)\n",
        "    precisions_classe.append(accuracy)\n",
        "precision_moyenne = np.mean(precisions_classe)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(noms_classe, precisions_classe)\n",
        "ax.plot([-0.5,9.5], [precision_moyenne, precision_moyenne], '--r', label='Précision moyenne')\n",
        "ax.set_xlabel('Classes')\n",
        "ax.set_ylabel(\"Précision pour chaque classe\")\n",
        "ax.set_title(\"Précision en fonction des classes dans l'ensemble de test\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6386K2kjuuRo"
      },
      "source": [
        "# Le Modèle PARKHI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NvAROULtNQG"
      },
      "outputs": [],
      "source": [
        "# create the neural network with Pytorch\n",
        "class P_Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "\n",
        "    S = 32\n",
        "\n",
        "    self.conv11 = nn.Conv2d(in_channels, S, kernel_size=3, padding=1)\n",
        "    self.bn11 = nn.BatchNorm2d(S)\n",
        "    self.conv12 = nn.Conv2d(S, S, kernel_size=3, padding=1)\n",
        "    self.bn12 = nn.BatchNorm2d(S)\n",
        "\n",
        "    self.conv21 = nn.Conv2d(S, 2*S, kernel_size=3, padding=1)\n",
        "    self.bn21 = nn.BatchNorm2d(2*S)\n",
        "    self.conv22 = nn.Conv2d(2*S, 2*S, kernel_size=3, padding=1)\n",
        "    self.bn22 = nn.BatchNorm2d(2*S)\n",
        "\n",
        "    self.conv31 = nn.Conv2d(2*S, 2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn31 = nn.BatchNorm2d(2*2*S)\n",
        "    self.conv32 = nn.Conv2d(2*2*S, 2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn32 = nn.BatchNorm2d(2*2*S)\n",
        "    self.conv33 = nn.Conv2d(2*2*S, 2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn33 = nn.BatchNorm2d(2*2*S)\n",
        "\n",
        "    self.conv41 = nn.Conv2d(2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn41 = nn.BatchNorm2d(2*2*2*S)\n",
        "    self.conv42 = nn.Conv2d(2*2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn42 = nn.BatchNorm2d(2*2*2*S)\n",
        "    self.conv43 = nn.Conv2d(2*2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn43 = nn.BatchNorm2d(2*2*2*S)\n",
        "\n",
        "    self.conv51 = nn.Conv2d(2*2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn51 = nn.BatchNorm2d(2*2*2*S)\n",
        "    self.conv52 = nn.Conv2d(2*2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn52 = nn.BatchNorm2d(2*2*2*S)\n",
        "    self.conv53 = nn.Conv2d(2*2*2*S, 2*2*2*S, kernel_size=3, padding=1)\n",
        "    self.bn53 = nn.BatchNorm2d(2*2*2*S)\n",
        "\n",
        "    self.fc6 = nn.Linear(in_features=2*2*2*S, out_features=4096)\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)  \n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "                  ## Stage 1\n",
        "    ## conv11 :\n",
        "    x = F.relu(self.bn11(self.conv11(x)))\n",
        "    ## conv12 :\n",
        "    x = F.relu(self.bn12(self.conv12(x)))\n",
        "    ## maxpool :\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "                  ## Stage 2\n",
        "    ## conv21 :\n",
        "    x = F.relu(self.bn21(self.conv21(x)))\n",
        "    ## conv22 :\n",
        "    x = F.relu(self.bn22(self.conv22(x)))\n",
        "    ## maxpool :\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "                  ## Stage 3\n",
        "    ## conv31 :\n",
        "    x = F.relu(self.bn31(self.conv31(x)))\n",
        "    ## conv32 :\n",
        "    x = F.relu(self.bn32(self.conv32(x)))\n",
        "    ## conv33 :\n",
        "    x = F.relu(self.bn33(self.conv33(x)))\n",
        "    ## maxpool :\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "                  ## Stage 4\n",
        "    ## conv41 :\n",
        "    x = F.relu(self.bn41(self.conv41(x)))\n",
        "    ## conv42 :\n",
        "    x = F.relu(self.bn42(self.conv42(x)))\n",
        "    ## conv43 :\n",
        "    x = F.relu(self.bn43(self.conv43(x)))\n",
        "    ## maxpool :\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "                  ## Stage 5\n",
        "    ## conv51 :\n",
        "    x = F.relu(self.bn51(self.conv51(x)))\n",
        "    ## conv52 :\n",
        "    x = F.relu(self.bn52(self.conv52(x)))\n",
        "    ## conv53 :\n",
        "    x = F.relu(self.bn53(self.conv53(x)))\n",
        "    ## maxpool :\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    ## flatten :\n",
        "    x = torch.flatten(x, 1)\n",
        "                  ## Stage 6\n",
        "    ## fully connected 6 :\n",
        "    x = F.relu(self.fc6(x))\n",
        "                  ## Stage 7\n",
        "    ## fully connected 7 :\n",
        "    x = F.relu(self.fc7(x))\n",
        "                  ## Stage 8\n",
        "    ## fully connected 8 :\n",
        "    x = F.relu(self.fc8(x))\n",
        "\n",
        "    return x\n",
        "  \n",
        "  def size_debug(self, x):\n",
        "        forward_source = inspect.getsource(self.forward)\n",
        "        forward_lines = forward_source.split('\\n')[1:-2]  # Skip def and return\n",
        "        local_vars = locals()\n",
        "        print(\"in :\")\n",
        "        print(x.shape)\n",
        "        for line in forward_lines:\n",
        "            if line.strip():\n",
        "                if line.strip()[0] != '#':\n",
        "                    exec(line.strip(), globals(), local_vars)\n",
        "                    x = local_vars['x']\n",
        "                    print(x.shape)\n",
        "                elif line.strip()[0:2] == '##':\n",
        "                    print(line.strip())\n",
        "        print(\"out.\")\n",
        "\n",
        "# actual network creation\n",
        "net = P_Net(3,3)\n",
        "net.cuda() # go GPU\n",
        "\n",
        "net.size_debug(images.cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8ngWEKjtXhA"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# # create the optimizer\n",
        "# optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "# criterion = F.mse_loss # ça marche pas ici parce que il faut criterion(outputs, labels) avec outputs et labels de meme size (pq ??)\n",
        "# criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQUAyrIJ8DiC"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_epochs = 2\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "PATH = f\"./cifar_net_parkhi_n_epochs={n_epochs}.pth\"\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tester le modele : (cette case peut être run autant de fois qu'on veut)\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "\n",
        "net = NetP(3,3)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "#net.cuda()\n",
        "\n",
        "outputs = net(images)\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tester sur tout le test dataset :\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
